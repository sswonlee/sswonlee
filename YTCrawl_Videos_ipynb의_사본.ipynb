{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sswonlee/sswonlee/blob/main/YTCrawl_Videos_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MIoVy8pWIzKX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c87a3023-7eb5-4b54-9b7f-3a32088ae987"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.4.0-py3-none-any.whl (985 kB)\n",
            "\u001b[K     |████████████████████████████████| 985 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting urllib3[secure,socks]~=1.26\n",
            "  Downloading urllib3-1.26.11-py2.py3-none-any.whl (139 kB)\n",
            "\u001b[K     |████████████████████████████████| 139 kB 38.4 MB/s \n",
            "\u001b[?25hCollecting trio~=0.17\n",
            "  Downloading trio-0.21.0-py3-none-any.whl (358 kB)\n",
            "\u001b[K     |████████████████████████████████| 358 kB 34.2 MB/s \n",
            "\u001b[?25hCollecting trio-websocket~=0.9\n",
            "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.10)\n",
            "Collecting async-generator>=1.9\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (22.1.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Collecting outcome\n",
            "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting wsproto>=0.14\n",
            "  Downloading wsproto-1.1.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (2022.6.15)\n",
            "Collecting cryptography>=1.3.4\n",
            "  Downloading cryptography-37.0.4-cp36-abi3-manylinux_2_24_x86_64.whl (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 41.4 MB/s \n",
            "\u001b[?25hCollecting pyOpenSSL>=0.14\n",
            "  Downloading pyOpenSSL-22.0.0-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (2.21)\n",
            "Collecting h11<1,>=0.9.0\n",
            "  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from h11<1,>=0.9.0->wsproto>=0.14->trio-websocket~=0.9->selenium) (4.1.1)\n",
            "Installing collected packages: sniffio, outcome, h11, cryptography, async-generator, wsproto, urllib3, trio, pyOpenSSL, trio-websocket, selenium\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.11 which is incompatible.\u001b[0m\n",
            "Successfully installed async-generator-1.10 cryptography-37.0.4 h11-0.13.0 outcome-1.2.0 pyOpenSSL-22.0.0 selenium-4.4.0 sniffio-1.2.0 trio-0.21.0 trio-websocket-0.9.2 urllib3-1.26.11 wsproto-1.1.0\n",
            "Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Get:3 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease [1,581 B]\n",
            "Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:10 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,528 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [903 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,905 kB]\n",
            "Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:17 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [2,077 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,368 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,310 kB]\n",
            "Get:20 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [1,064 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [1,141 kB]\n",
            "Get:22 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [45.3 kB]\n",
            "Fetched 15.6 MB in 3s (4,637 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 27 not upgraded.\n",
            "Need to get 90.4 MB of archives.\n",
            "After this operation, 306 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 103.0.5060.134-0ubuntu0.18.04.1 [1,160 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 103.0.5060.134-0ubuntu0.18.04.1 [79.0 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 103.0.5060.134-0ubuntu0.18.04.1 [5,043 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 103.0.5060.134-0ubuntu0.18.04.1 [5,202 kB]\n",
            "Fetched 90.4 MB in 2s (45.2 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 155680 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_103.0.5060.134-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (103.0.5060.134-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_103.0.5060.134-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (103.0.5060.134-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_103.0.5060.134-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (103.0.5060.134-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_103.0.5060.134-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (103.0.5060.134-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (103.0.5060.134-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (103.0.5060.134-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (103.0.5060.134-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (103.0.5060.134-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.5) ...\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n"
          ]
        }
      ],
      "source": [
        "!pip install selenium\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nGSpf29EI4hd"
      },
      "outputs": [],
      "source": [
        "from selenium import webdriver\n",
        "import time\n",
        "from openpyxl import Workbook\n",
        "import pandas as pd\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Asb1kLaNI7ns",
        "outputId": "4557bca5-78fe-4ac1-b6c1-c9f656e4bbec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (3.0.4) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.0.3-py3-none-any.whl (149 kB)\n",
            "\u001b[K     |████████████████████████████████| 149 kB 5.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.0.3\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: UTF-8 -*-\n",
        "import time\n",
        "from selenium import webdriver\n",
        "import requests\n",
        "!pip install xlsxwriter\n",
        "#Colab에선 웹브라우저 창이 뜨지 않으므로 별도 설정한다.\n",
        " \n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')        # Head-less 설정\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NH70ENW4JFca"
      },
      "outputs": [],
      "source": [
        "from selenium.webdriver.common.by import By"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRm_8wm6aCHl",
        "outputId": "43c12d46-b29f-4b89-8844-455bc4fc4b94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type3.txt\n",
            "\n",
            "metaverse copyright problem\n",
            "\n",
            "metaverse sex offense\n",
            "\n",
            "metaverse bullying\n",
            "\n",
            "money laundering in metaverse\n",
            "\n",
            "------------------------------\n",
            "Crawling about keyword: metaverse copyright problem\n",
            "\n"
          ]
        }
      ],
      "source": [
        "TYPE = 1\n",
        "KEY = 0\n",
        "\n",
        "type_n = open('type'+str(TYPE)+ \".txt\").readlines()\n",
        "print(\"type3.txt\\n\")\n",
        "for search_key in type_n:\n",
        "  print(search_key)\n",
        "search = type_n[KEY]\n",
        "print(\"-\"*30)\n",
        "print(\"Crawling about keyword: \" + search)\n",
        "# search_data = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8WjIUdORbCxD"
      },
      "outputs": [],
      "source": [
        "url = \"https://www.youtube.com/results?search_query=\" + search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bDa9LiRibIiu"
      },
      "outputs": [],
      "source": [
        "driver = webdriver.Chrome('chromedriver', options=options)\n",
        "driver.get(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hsj8hXxubVSn"
      },
      "outputs": [],
      "source": [
        "last_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
        "while True:\n",
        "    driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
        "    time.sleep(1.5)\n",
        "\n",
        "    new_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
        "    if new_height == last_height:\n",
        "        break\n",
        "    last_height = new_height\n",
        "\n",
        "    if new_height > 3000: #동영상은 꽤 많이 나와서, 페이지 길이에 한계를 설정\n",
        "      break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Yd-hr0b3be-E"
      },
      "outputs": [],
      "source": [
        "html = driver.page_source\n",
        "soup = BeautifulSoup(html, 'html.parser')\n",
        "video_list = soup.select(\"a#video-title\") #video 정보가 포함된 태그들의 리스트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "435WPOdBbhc-"
      },
      "outputs": [],
      "source": [
        "id, titles = [], []\n",
        "for video in video_list: \n",
        "  id.append(video['href']) #동영상 ID(링크)\n",
        "  titles.append(video['title']) #동영상 제목\n",
        "channel_id_list = soup.select('div#channel-info > a')\n",
        "# views_list = soup.select(\"div#metadata > div#metadata-line > span:nth-of-type(1)\")\n",
        "# time_list = soup.select(\"div#metadata > div#metadata-line > span:nth-of-type(2)\")\n",
        "\n",
        "# for i in range(len(views_list)):\n",
        "#   views_list[i] = views_list[i].text\n",
        "\n",
        "# for i in range(len(time_list)):\n",
        "#   time_list[i] = time_list[i].text\n",
        "\n",
        "#영상 id, 제목, 조회수, 업로드 시간"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_bgARhvkiDc",
        "outputId": "507d1ac4-4522-40a1-99e2-448c97a6630e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['2021-11-13', '2021-12-07', '2022-03-17', '2021-12-30', '2022-01-21', '2022-01-28', '2022-06-17', '2021-08-11', '2021-11-22', '2021-12-14', '2022-01-06', '2021-03-15', '2022-08-09', '2022-08-09', '2022-08-09', '2022-02-07', '2021-11-10', '2022-02-15', '2022-02-15', '2021-07-01', '2021-11-07', '2021-03-18', '2021-09-21', '2022-04-11', '2022-08-04', '2022-04-27', '2021-11-03', '2019-01-26', '2021-11-12', '2022-08-09', '2022-02-07', '2021-11-26', '2021-12-08', '2021-06-18', '2021-10-31', '2022-04-16', '2022-02-18', '2022-04-21', '2011-03-24', '2021-12-09', '2017-03-15', '2022-04-20', '2022-04-14', '2022-07-01', '2022-03-04', '2022-02-09', '2022-03-15', '2022-07-07', '2022-03-22', '2022-01-15', '2022-02-07', '2021-11-11', '2022-02-02', '2022-07-19', '2022-07-15', '2022-01-21', '2022-02-14', '2022-04-11', '2021-03-24', '2021-03-19', '2022-07-10', '2022-07-19', '2021-04-23', '2022-07-14', '2022-07-18', '2022-01-11', '2022-07-15', '2022-04-30', '2021-03-11', '2022-04-01', '2022-07-14', '2021-11-10', '2022-03-22', '2022-07-13', '2022-04-28', '2022-03-30', '2022-03-16', '2021-11-10', '2021-12-01', '2022-05-27']\n"
          ]
        }
      ],
      "source": [
        "views_list = []\n",
        "dates_list = []\n",
        "for vid_id in id:\n",
        "  vid_url = 'https://www.youtube.com'+ vid_id\n",
        "  vid_html = requests.get(vid_url).text\n",
        "  vid_soup = BeautifulSoup(vid_html, 'html.parser')\n",
        "  \n",
        "  views = vid_soup.find('meta', itemprop='interactionCount')\n",
        "  views_list.append(views.text)\n",
        "\n",
        "  date = vid_soup.find('meta', itemprop = 'uploadDate')\n",
        "  dates_list.append(date['content'])\n",
        "\n",
        "print(dates_list)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build"
      ],
      "metadata": {
        "id": "gLHTNv2KhUkq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 896
        },
        "id": "pvqXKA4WgniC",
        "outputId": "4a8cad30-0e6e-44fb-82e4-0dd66acd6763"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:googleapiclient.discovery_cache:file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 33, in <module>\n",
            "    from oauth2client.contrib.locked_file import LockedFile\n",
            "ModuleNotFoundError: No module named 'oauth2client.contrib.locked_file'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 37, in <module>\n",
            "    from oauth2client.locked_file import LockedFile\n",
            "ModuleNotFoundError: No module named 'oauth2client.locked_file'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 44, in autodetect\n",
            "    from . import file_cache\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 41, in <module>\n",
            "    \"file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\"\n",
            "ImportError: file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "HttpError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-e93a094e696a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m   search_response = youtube.search().list(\n\u001b[1;32m     15\u001b[0m     \u001b[0mchannelId\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchannel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mpart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'id'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     ).execute()\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/googleapiclient/_helpers.py\u001b[0m in \u001b[0;36mpositional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mpositional_parameters_enforcement\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPOSITIONAL_WARNING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpositional_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/googleapiclient/http.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    913\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHttpError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHttpError\u001b[0m: <HttpError 400 when requesting https://youtube.googleapis.com/youtube/v3/search?channelId=%3Ca+aria-label%3D%22Go+to+channel%22+class%3D%22style-scope+ytd-video-renderer%22+href%3D%22%2Fc%2FMrwhosetheboss%22+id%3D%22channel-thumbnail%22%3E%0A%3Cyt-img-shadow+class%3D%22style-scope+ytd-video-renderer+no-transition%22+loaded%3D%22%22+style%3D%22background-color%3A+transparent%3B%22+width%3D%2224%22%3E%3C%21--css_build_mark%3Avideo.youtube.src.web.polymer.shared.ui.yt_img_shadow.yt.img.shadow.css.js--%3E%3C%21--css_build_scope%3Ayt-img-shadow--%3E%3C%21--css_build_styles%3Avideo.youtube.src.web.polymer.shared.ui.styles.yt_base_styles.yt.base.styles.css.js%2Cvideo.youtube.src.web.polymer.shared.ui.yt_img_shadow.yt.img.shadow.css.js--%3E%3Cimg+alt%3D%22%22+class%3D%22style-scope+yt-img-shadow%22+draggable%3D%22false%22+id%3D%22img%22+src%3D%22https%3A%2F%2Fyt3.ggpht.com%2FIkb1C4ih2VMvfjma8OO5b39JnHL2CQcQgksB_I7TM-gGA3ERTY589OeLKCYyRQQO0nkE54-f%3Ds68-c-k-c0x00ffffff-no-rj%22+width%3D%2224%22%2F%3E%3C%2Fyt-img-shadow%3E%0A%3C%2Fa%3E&part=id&key=AIzaSyDkB8myIjs1zPQ8j34vYcOEIG5QnuXL7Lo&alt=json returned \"Request contains an invalid argument.\". Details: \"Request contains an invalid argument.\">"
          ]
        }
      ],
      "source": [
        "subs = []\n",
        "DEVELOPER_KEY='AIzaSyDkB8myIjs1zPQ8j34vYcOEIG5QnuXL7Lo' # 내 API 키값 입력\n",
        "YOUTUBE_API_SERVICE_NAME='youtube'\n",
        "YOUTUBE_API_VERSION='v3'\n",
        "youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION ,developerKey = DEVELOPER_KEY)\n",
        "\n",
        "for channel_id in channel_id_list:\n",
        "  # try:\n",
        "  channel_url = \"https://www.youtube.com\" + channel_id['href']\n",
        "  # channel_html = requests.get(channel_url).text\n",
        "  # channel_soup = BeautifulSoup(channel_html, 'html.parser')\n",
        "  # sub_num =  channel_soup.select('.subscriber_cnt')[0].text \n",
        "  # print(sub_num)\n",
        "  search_response = youtube.search().list(\n",
        "    channelId = channel_id,\n",
        "    part = 'id'\n",
        "    ).execute()\n",
        "\n",
        "print(subs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_17qaL4qJIB_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fb3eb90-0386-459e-e851-145b293b9582"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['metaverse copyright problem\\n', 'metaverse sex offense\\n', 'metaverse bullying\\n', 'money laundering in metaverse\\n']\n",
            "metaverse copyright problem\n",
            "\n",
            "5422\n",
            "['/c/Mrwhosetheboss', '/c/VICENews', '/c/TheDailyShow', '/channel/UC7SAafuLaYXju9argWf8gcQ', '/c/FoldingIdeas', '/c/ThenNow', '/c/JaySharon', '/c/IanCorzine-Truth', '/channel/UCq76OnPaXOnvo73sECoUcBQ', '/c/sciencespo', '/c/ColinandSamir', '/c/joerogan', '/c/RicoTrades', '/channel/UCfdrZpVbXl_HnmyYYo-N6Ig', '/c/JohnCooganPlus', '/c/joerogan', '/c/Lawfulmassesunite', '/c/TheInfographicsShowOFFICIAL', '/c/Moon-Real', '/c/GeographyHub', '/channel/UCTrlF6GUUXlzeb0yz7m_2jg', '/c/IanCorzine-Truth', '/c/CSUCMLAWSchool', '/c/SteveVondran', '/c/BernardMarrAPI', '/user/PhillyLikwid', '/c/EddyBurback', '/c/TheUplandShow', '/c/JilleneD', '/c/TheHatedOne', '/c/wsj', '/c/LegalEagle', '/c/TheBigPicturewithDrJowallah', '/channel/UC0SW32ROXBTjZnqBOXY6ozA', '/channel/UCfdrZpVbXl_HnmyYYo-N6Ig', '/channel/UCyD59CI7beJDU493glZpxgA', '/c/JohnCooganPlus', '/c/ChrisHarounVenturesCompleteBusinessEducation', '/c/YongYea', '/c/CBSMornings', '/c/WION', '/c/TechnicalGuruji', '/c/allupinyobiz', '/c/ChristaLaser']\n",
            "44\n",
            "종료\n",
            "44\n",
            "종료\n",
            "44\n",
            "종료\n",
            "44\n",
            "종료\n",
            "44\n",
            "종료\n",
            "44\n",
            "종료\n",
            "44\n",
            "종료\n",
            "44\n",
            "종료\n",
            "44\n",
            "종료\n",
            "44\n",
            "종료\n",
            "44\n",
            "종료\n",
            "44\n",
            "종료\n",
            "44\n",
            "종료\n",
            "44\n",
            "종료\n",
            "44\n",
            "종료\n",
            "44\n",
            "종료\n",
            "44\n",
            "종료\n",
            "44\n",
            "종료\n",
            "44\n",
            "종료\n",
            "44\n",
            "종료\n",
            "44\n",
            "종료\n",
            "44\n",
            "종료\n",
            "44\n",
            "종료\n",
            "44\n",
            "종료\n",
            "44\n",
            "종료\n",
            "44\n",
            "종료\n"
          ]
        }
      ],
      "source": [
        "#해당 url로 이동\n",
        "pd_metadata = {}\n",
        "pd_comments = pd.DataFrame({})\n",
        "for t in range(1, 2):\n",
        "  type_n = open('type'+str(t)+\".txt\").readlines()\n",
        "  print(type_n)\n",
        "  search_data = {}\n",
        "  for search in type_n[0:]:\n",
        "    print(search)\n",
        "    driver = webdriver.Chrome('chromedriver', options=options)\n",
        "    url = \"https://www.youtube.com/results?search_query=\" + search #search 단어를 검색했을 때 나오는 페이지\n",
        "    driver.get(url)\n",
        "    driver.execute_script(\"window.scrollTo(0,1000)\")\n",
        "    time.sleep(3)\n",
        "\n",
        "    # 페이지 끝까지 스크롤(복사)\n",
        "    last_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
        "    print(last_height)\n",
        "    while True:\n",
        "        driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
        "        time.sleep(1.5)\n",
        "\n",
        "        new_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
        "        if new_height == last_height:\n",
        "            break\n",
        "        last_height = new_height\n",
        "\n",
        "        if new_height > 1000: #동영상은 꽤 많이 나와서, 페이지 길이에 한계를 설정\n",
        "          break\n",
        "    html = driver.page_source\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    video_list = soup.select(\"a#video-title\")\n",
        "\n",
        "    id, titles = [], []\n",
        "    for video in video_list: \n",
        "      id.append(video['href']) #동영상 ID(링크)\n",
        "      titles.append(video['title']) #동영상 제목\n",
        "    views, channels, subs = [], [], []\n",
        "    channel_id_list = soup.select('div#channel-info > a')\n",
        "    views_list = soup.select(\"div#metadata > div > span:nth-of-type(1)\")\n",
        "    time_list = soup.select(\"div#metadata > div > span:nth-of-type(2)\")\n",
        "\n",
        "    for channel_id in channel_id_list:\n",
        "      try:\n",
        "        channels.append(channel_id['href'])\n",
        "        channel_url = \"https://www.youtube.com\" + channel_id['href']\n",
        "        channel_html = requests.get(channel_url).text\n",
        "        channel_soup = BeautifulSoup(channel_html, 'html.parser')\n",
        "        subs.append(channel_soup.select('#subscriber-count').text)\n",
        "      except:\n",
        "        subs.append(-1)\n",
        "    print(channels)\n",
        "    for v in views_list:\n",
        "      views.append(v.text)\n",
        "    \n",
        "    id_final = []\n",
        "    comment_final = []\n",
        "    likes_final = []\n",
        "    video_comments_data = {}\n",
        "    for vid_idx in range(len(id)):\n",
        "      video_id = id[vid_idx]\n",
        "      url = \"https://www.youtube.com\" + video_id\n",
        "      try:\n",
        "        driver.get(url)\n",
        "      except:\n",
        "        break\n",
        "      driver.implicitly_wait(3)\n",
        "\n",
        "      time.sleep(1.5)\n",
        "\n",
        "      driver.execute_script(\"window.scrollTo(0,1000)\")\n",
        "      time.sleep(3)\n",
        "\n",
        "      # 페이지 끝까지 스크롤\n",
        "      last_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
        "\n",
        "      while True:\n",
        "          driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
        "          time.sleep(1.5)\n",
        "\n",
        "          new_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
        "          if new_height == last_height:\n",
        "              break\n",
        "          last_height = new_height\n",
        "          if new_height > 30000:\n",
        "            break\n",
        "\n",
        "      time.sleep(1.5)\n",
        "\n",
        "      # 팝업 닫기\n",
        "      try:\n",
        "          driver.find_elements(\"#dismiss-button > a\").click()\n",
        "      except:\n",
        "          pass\n",
        "\n",
        "      # 대댓글 모두 열기\n",
        "      buttons = driver.find_elements(By.ID, \"more-replies\")\n",
        "\n",
        "      time.sleep(1.5)\n",
        "\n",
        "      for button in buttons:\n",
        "          try:\n",
        "              button.send_keys(Keys.ENTER)\n",
        "          except:\n",
        "            pass\n",
        "          time.sleep(1.5)\n",
        "          try:\n",
        "            button.click()\n",
        "          except:\n",
        "            pass\n",
        "\n",
        "      time.sleep(1.5)\n",
        "\n",
        "      # 정보 추출하기\n",
        "      html_source = driver.page_source\n",
        "      soup = BeautifulSoup(html_source, 'html.parser')\n",
        "\n",
        "      id_list = soup.select(\"div#header-author > h3 > #author-text > span\")\n",
        "      comment_list = soup.select(\"yt-formatted-string#content-text\")\n",
        "      likes_list = soup.select('#vote-count-middle')\n",
        "      for i in range(len(comment_list)):\n",
        "        temp_id = id_list[i].text\n",
        "        temp_id = temp_id.replace('\\n', '')\n",
        "        temp_id = temp_id.replace('\\t', '')\n",
        "        temp_id = temp_id.replace('    ', '')\n",
        "        id_final.append(temp_id)\n",
        "\n",
        "        temp_comment = comment_list[i].text\n",
        "        temp_comment = temp_comment.replace('\\n', '')\n",
        "        temp_comment = temp_comment.replace('\\t', '')\n",
        "        temp_comment = temp_comment.replace('    ', '')\n",
        "        comment_final.append(temp_comment)\n",
        "\n",
        "        likes_final.append(likes_list[i].text)\n",
        "      print(len(subs))\n",
        "      pd_comments = pd.concat([pd_comments, pd.DataFrame({'타입': [t for temp in range(len(id_final))], '검색어': [search for temp in range(len(id_final))],'영상': [video_id for temp in range(len(id_final))], \\\n",
        "                                                          #'구독자': [subs[vid_idx] for temp in range(len(id_final))],\\\n",
        "                                                          '작성자': id_final, '내용': comment_final\\\n",
        "                                                          ,'좋아요': likes_final\n",
        "                                                          })])\n",
        "      print(\"종료\")\n",
        "\n",
        "  # pd_metadata['type' + str(i)] = search_data\n",
        "print(pd_comments)\n",
        "pd.DataFrame(pd_comments).to_excel(\"comments.xlsx\")\n",
        "# pd_metadata = pd.DataFrame(pd_metadata)\n",
        "# pd_metadata.to_excel('metadata.xlsx')\n",
        "driver.quit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLlSh7ntHyHu"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CILQbvTiTkow"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "YTCrawl_Videos.ipynb의 사본",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}